{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import igl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from preprocess import mesh_sampling_method\n",
    "from dataset import MeshData\n",
    "from models import VAE_coma, GraphPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the training and testing dataset...\n",
      "Normalization Done!\n"
     ]
    }
   ],
   "source": [
    "# read into the config file\n",
    "config_path = 'config/general_config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# set the device, we can just assume we are using single GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = config[\"dataset\"]\n",
    "template = config[\"template\"]\n",
    "data_dir = osp.join('data', dataset)\n",
    "template_fp = osp.join('template', template)\n",
    "\n",
    "# get the up/down sampling matrix\n",
    "ds_factor = config[\"ds_factor\"]\n",
    "edge_index_list, down_transform_list, up_transform_list = mesh_sampling_method(data_fp=data_dir,\n",
    "                                                                                template_fp=template_fp,\n",
    "                                                                                ds_factors=ds_factor,\n",
    "                                                                                device=device)\n",
    "\n",
    "# create the model\n",
    "in_channels = config[\"model\"][\"in_channels\"]\n",
    "out_channels = config[\"model\"][\"out_channels\"]\n",
    "latent_channels = config[\"model\"][\"latent_channels\"]\n",
    "K = config[\"model\"][\"K\"]\n",
    "\n",
    "# get the mean and std of the CAESAR dataset(Traing set)\n",
    "CAESAR_meshdata = MeshData(root=data_dir, template_fp=template_fp)\n",
    "# of shape (10002, 3). is torch.Tensor\n",
    "caesar_mean = CAESAR_meshdata.mean\n",
    "caesar_std = CAESAR_meshdata.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the predictor and the CoMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = GraphPredictor(in_channels = in_channels, out_channels = out_channels,\n",
    "                        edge_index = edge_index_list, down_transform = down_transform_list, K=K).to(device)\n",
    "predictor.load_state_dict(torch.load(\"predictor_network/predictor.pth\"))\n",
    "\n",
    "height_mean = 1716.4373\n",
    "height_std = 107.98842\n",
    "\n",
    "arm_length_mean = 612.611\n",
    "arm_length_std = 45.986\n",
    "\n",
    "crotch_height_mean = 773.540\n",
    "crotch_height_std = 55.731\n",
    "\n",
    "chest_mean = 996.745\n",
    "chest_std = 124.099\n",
    "\n",
    "hip_mean = 1050.170\n",
    "hip_std = 113.026\n",
    "\n",
    "waist_mean = 848.005\n",
    "waist_std = 144.338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE_coma(in_channels = in_channels,\n",
    "                out_channels = out_channels,\n",
    "                latent_channels = latent_channels,\n",
    "                edge_index = edge_index_list,\n",
    "                down_transform = down_transform_list,\n",
    "                up_transform = up_transform_list,\n",
    "                K=K).to(device)\n",
    "\n",
    "\n",
    "# model_path = \"out/vae_coma/trainer12/20231220-123538/model.pth\" # our method\n",
    "# model_path = \"out/vae_coma/trainer13/20240119-003904/model.pth\" # our method without Laplacian loss\n",
    "model_path = \"out/vae_coma/trainer14/20240119-090527/model.pth\" # our method without AWL\n",
    "# model_path = \"out/vae_coma/trainer15/20240119-193650/model.pth\" # our method without disentanglement\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the decoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_func(x, model):\n",
    "    num_layers = len(model.de_layers)\n",
    "    num_deblocks = num_layers - 2\n",
    "    for i, layer in enumerate(model.de_layers):\n",
    "        if i == 0:\n",
    "            x = layer(x)\n",
    "            x = x.view(-1, model.num_vert, model.out_channels[-1])\n",
    "        elif i != num_layers - 1:\n",
    "            x = layer(x, model.edge_index[num_deblocks - i],\n",
    "                        model.up_transform[num_deblocks - i])\n",
    "        else:\n",
    "            # last layer\n",
    "            x = layer(x, model.edge_index[0])\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the computation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We need further modification to this function\n",
    "def get_feature_vals(model, sample_vals: np.ndarray, mean: np.ndarray, std: np.ndarray, predictor):\n",
    "    \n",
    "    \"\"\"_summary_\n",
    "        load the model from model_path\n",
    "        sample points from the latent space\n",
    "        return the latent values and the corresponding height values and weight values\n",
    "\n",
    "    Args:\n",
    "        model: the generative model\n",
    "        sample_vals (np.ndarray): of shape (n_samples, ). We would sample n_samples from the latent space. And use the same vals for all 8 dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample from the latent space\n",
    "    # create a tensor of shape (n_samples, 8, 6)\n",
    "    feature_vals = np.zeros((sample_vals.shape[0], 8, 6))\n",
    "\n",
    "    # for each latent dimension\n",
    "    for i in tqdm(range(8)):\n",
    "        latent_val = torch.zeros((sample_vals.shape[0], 8))\n",
    "\n",
    "        latent_val[:, i] = torch.Tensor(sample_vals)\n",
    "        latent_val = latent_val.to(device) \n",
    "        \n",
    "        # decode the latent values\n",
    "        # of shape (n_samples, 10002, 3)\n",
    "        v = decode_func(latent_val, model)\n",
    "        \n",
    "        # get the weight and height of the mesh\n",
    "        features = predictor(v)\n",
    "\n",
    "        # features is of shape (n_samples, 6)\n",
    "        # mean is of shape (6, ), std is of shape (6, )\n",
    "        features = features.cpu().detach().numpy()\n",
    "        features = features * std + mean\n",
    "       \n",
    "        feature_vals[:, i, :] = features\n",
    "        \n",
    "    return feature_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We need furthur modification to the plot_latent function\n",
    "def plot_latent(filename: str, sample_vals: np.ndarray, feature_vals: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "        declare a plot of 6 subplots, 6 rows and 1 column\n",
    "        each subplot is a dot plot of the corresponding latent dimension\n",
    "        the x-axis is the latent value, the y-axis is the height\n",
    "        the figure size is tight to the subplots\n",
    "    Args:\n",
    "        filename (str): the file name to save the plot.\n",
    "        sample_vals (np.ndarray): of shape (n_samples, ). We would sample n_samples from the latent space. And use the same vals for all 8 dimensions.\n",
    "        feature_vals (np.ndarray): of shape (n_samples, 8, 6). 8 is the number of latent dimensions. 6 is the number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the dimensions match up correctly\n",
    "    if len(sample_vals) != feature_vals.shape[0] or feature_vals.shape[1] != 8 or feature_vals.shape[2] != 6:\n",
    "        raise ValueError(\"Input dimensions are mismatched!\")\n",
    "\n",
    "    labels = [\"Dim1\", \"Dim2\", \"Dim3\", \"Dim4\", \"Dim5\", \"Dim6\", \"Dim7\", \"Dim8\"]\n",
    "    lines = []\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(5, 20), sharex=True)\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        \n",
    "        for j in range(8):\n",
    "            line, = ax.plot(sample_vals, feature_vals[:, j, i], label=labels[j])\n",
    "        \n",
    "            if i == 0:\n",
    "                lines.append(line)\n",
    "        ax.set_xlim(-3, 3)\n",
    "\n",
    "\n",
    "    # first create a dummy legend, so fig.tight_layout() makes enough space\n",
    "    axs[0].legend(handles=lines, labels = labels, ncol=4,\n",
    "                    bbox_to_anchor=(0.5, 1.0), loc='lower center')\n",
    "    fig.tight_layout()\n",
    "    # # now create the real legend; if fig.tight_layout() were called on this,\n",
    "    # #  it would create a large empty space between the columns of subplots\n",
    "    # #  as it wants the legend to belong to only one of the subplots\n",
    "    # axs[0].legend(handles=lines, labels = labels, ncol=4,\n",
    "    #                 bbox_to_anchor=(1.03, 1.12), loc='lower center', fontsize=8)\n",
    "\n",
    "    # Save the figure to the given filename\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    # Close the figure\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 22.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_vals = np.linspace(-3, 3, 50)\n",
    "\n",
    "mean = np.array([height_mean, arm_length_mean, crotch_height_mean, chest_mean, hip_mean, waist_mean])\n",
    "std = np.array([height_std, arm_length_std, crotch_height_std, chest_std, hip_std, waist_std])\n",
    "\n",
    "feature_vals = get_feature_vals(model, sample_vals, mean, std, predictor)\n",
    "\n",
    "plot_latent(\"result/together/wo_awl_features.pdf\", sample_vals, feature_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate mesh samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read into the template mesh\n",
    "v, f = igl.read_triangle_mesh(template_fp)\n",
    "\n",
    "\n",
    "def generate_a_mesh(model, latent_vector: np.ndarray, mean, std, fp, name):\n",
    "    mean = mean.to(device)\n",
    "    std = std.to(device)\n",
    "    \n",
    "    latent_vector = torch.Tensor(latent_vector).cuda()\n",
    "    latent_vector = latent_vector.reshape(1, -1)\n",
    "    \n",
    "    v = decode_func(latent_vector, model)\n",
    "    v = v.detach()\n",
    "    v = v * std + mean\n",
    "    v = v.cpu().numpy()\n",
    "    v = v.reshape(-1, 3)\n",
    "    \n",
    "    # find the lowest vertex\n",
    "    z_min = v[:, 2].min()\n",
    "    # move the lowest vertex to the origin\n",
    "    v[:, 2] = v[:, 2] - z_min\n",
    "        \n",
    "    if not osp.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "    \n",
    "    igl.write_triangle_mesh(osp.join(fp, name), v, f)\n",
    "\n",
    "    \n",
    "\n",
    "# generate a banch of meshes samples from the latent space\n",
    "def generate_mesh_samples(model, latent_vals: np.ndarray, mean, std, latent_dim):\n",
    "    \"\"\"_summary_\n",
    "        generate meshes from the latent values\n",
    "        save the meshes to the result folder\n",
    "\n",
    "    Args:\n",
    "        model_path (str): the path to the model file\n",
    "        latent_vals (np.ndarray): of shape (n_samples,). The latent values for the first latent dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # move the mean and std to the device\n",
    "    mean = mean.to(device)\n",
    "    std = std.to(device)\n",
    "\n",
    "\n",
    "    # create a tensor of shape (n_samples, 8)\n",
    "    latent_vectors = np.zeros((latent_vals.shape[0], 8))\n",
    "    # set the latent dimension to be latent_vals\n",
    "    latent_vectors[:, latent_dim] = latent_vals\n",
    "    # convert to torch.Tensor\n",
    "    latent_vectors = torch.Tensor(latent_vectors).cuda()\n",
    "\n",
    "    # decode the latent values\n",
    "    # of shape (n_samples, 10002, 3)\n",
    "    v = decode_func(latent_vectors, model)\n",
    "    # convert to numpy array\n",
    "    v = v.detach()\n",
    "    # denormalize the vertices\n",
    "    v = v * std + mean\n",
    "\n",
    "    v = v.cpu().numpy()\n",
    "\n",
    "    # save the meshes\n",
    "    for i in range(v.shape[0]):\n",
    "        vertex = v[i]\n",
    "        # find the lowest vertex\n",
    "        z_min = vertex[:, 2].min()\n",
    "        # move the lowest vertex to the origin\n",
    "        vertex[:, 2] = vertex[:, 2] - z_min\n",
    "\n",
    "        # check if the folder exists\n",
    "        if not osp.exists(\"result/meshes/latent{}\".format(latent_dim)):\n",
    "            os.makedirs(\"result/meshes/latent{}\".format(latent_dim))\n",
    "        \n",
    "        igl.write_triangle_mesh(\"result/meshes/latent{}/{:.1f}.obj\".format(latent_dim, latent_vals[i]), vertex, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vals = np.array((-6, -4, -2, 0, 2, 4, 6))\n",
    "# generate_mesh_samples(model=model, latent_vals=latent_vals, mean=caesar_mean, std=caesar_std, latent_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vector = np.zeros(8)\n",
    "# values = [-4.5, -3, -1.5, 0, 1.5, 3, 4.5]\n",
    "\n",
    "# for i in range(len(values)):\n",
    "#     latent_vector[1] = values[i]\n",
    "#     latent_vector[2] = values[i]\n",
    "#     latent_vector[3] = values[i]\n",
    "#     generate_a_mesh(model, latent_vector, caesar_mean, caesar_std, \"result/meshes/teaser\", \"{:.1f}.obj\".format(values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vector = np.array((7, 0, 0, 0, 7, -7, 0, 0))\n",
    "# generate_a_mesh(model=model, latent_vector=latent_vector, mean=caesar_mean, std=caesar_std, fp=\"result/meshes/extreme\", name=\"reason_L.obj\")\n",
    "\n",
    "# latent_vector = np.array((-7, 0, 0, 0, -7, 7, 0, 0))\n",
    "# generate_a_mesh(model=model, latent_vector=latent_vector, mean=caesar_mean, std=caesar_std, fp=\"result/meshes/extreme\", name=\"reason_H.obj\")\n",
    "\n",
    "# latent_vector = np.array((-7, 0, 0, 0, 7, -7, 0, 0))\n",
    "# generate_a_mesh(model=model, latent_vector=latent_vector, mean=caesar_mean, std=caesar_std, fp=\"result/meshes/extreme\", name=\"extreme_H.obj\")\n",
    "\n",
    "# latent_vector = np.array((7, 0, 0, 0, -7, 7, 0, 0))\n",
    "# generate_a_mesh(model=model, latent_vector=latent_vector, mean=caesar_mean, std=caesar_std, fp=\"result/meshes/extreme\", name=\"extreme_L.obj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "body_coma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
