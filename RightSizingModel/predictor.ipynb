{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "1. The data we are using now has been preprocessed. This means every mesh in the `human-body2.zip` has corresponding label, like `height`, `weight`, `gender`, `arm length` and so on.\n",
    "2. If the data is preprocessed, u can do either way:\n",
    "   1. Replace the `Models20K` with the new data.\n",
    "   2. Delete all the files in `data/human-body2/raw/`. Compress the new data as `human-body2.zip` and move it into `data/human-body2/raw/`.\n",
    "3. If the data is not preprocessed, u can do the following:\n",
    "   1. Move the original `human-body2.zip` to `data/human-body2/raw/` and extract the `.zip` file there. \n",
    "   2. Run the `label-process.ipynb`. This would give us a new `metric.csv` file.\n",
    "   3. Pick the `.csv` file you want. Here we choose the `matric_standardization.csv`. We rename it and move the `metric.csv` file to the `data/human-body2/raw/`.\n",
    "   4. In this case, the `human-body2.zip` is still the original version. But it doesn't affect, as we have deleted the mesh with NaN in the `Models20K`.\n",
    "4. ALWAYS REMEMBER: The content in the `Models20K` does really matter. So, we need to make sure, they are really preprocessed. After preprocessing, there should be 2161 files in total. (2169 before preprocessing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading datas...\n",
      "Normalizing the training and testing dataset...\n",
      "Normalization Done!\n",
      "Data Loading finishes!\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from dataset import MeshData, DataLoader\n",
    "\n",
    "\n",
    "data_dir = osp.join('data', 'human-body2')\n",
    "template_fp = osp.join('template', 'template2.ply')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Start Loading datas...\")\n",
    "meshdata = MeshData(data_dir, template_fp)\n",
    "train_loader = DataLoader(meshdata.train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(meshdata.test_dataset, batch_size=batch_size)\n",
    "print(\"Data Loading finishes!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3407 # Torch.manual_seed(3407) is all you need\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoMA Based Predictor\n",
    "\n",
    "### Different Training Losses\n",
    "\n",
    "**For L2 Loss**\n",
    "\n",
    "1. If we just use CoMA structure, after 250 epochs, the loss is around 3\n",
    "2. If we still use CoMA structure, but we stopped at 130 epochs, the loss is 2.5.\n",
    "3. If we add some linear layers right after the CoMA structure, after 250 epochs, the loss is around 3.5.\n",
    "   \n",
    "**For L1 Loss**\n",
    "Compared with L2 Loss, L1 Loss would not penalize heavily to the outliers. If we still use CoMA structure, but we use L1 Loss, the result has no much difference with the one using L2 Loss. \n",
    "\n",
    "### Different Training Mode\n",
    "\n",
    "Previously, we tried two different training mode: Separate Training and Combined Training. **Separate Training** means for each attribtues, we design a network to predict it. **Combined Training** means we design a network to predict all the attributes at the same time.\n",
    "\n",
    "We tried both of them in my thesis work. But the result has no much difference. To make things easier in this Jupyter Notebook, we just use the **Combined Training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuliwuli/Desktop/predictor-net-measure/preprocess/mesh_sampling.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  torch.LongTensor([spmat.tocoo().row,\n"
     ]
    }
   ],
   "source": [
    "from models import Enblock\n",
    "from preprocess import mesh_sampling_method\n",
    "\n",
    "class GraphPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_index, down_transform, K, **kwargs):\n",
    "        super(GraphPredictor, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_index = edge_index\n",
    "        self.down_transform = down_transform\n",
    "        # self.num_vert used in the last and the first layer of encoder and decoder\n",
    "        self.num_vert = self.down_transform[-1].size(0)\n",
    "\n",
    "        # encoder\n",
    "        self.en_layers = nn.ModuleList()\n",
    "        for idx in range(len(out_channels)):\n",
    "            if idx == 0:\n",
    "                self.en_layers.append(\n",
    "                    Enblock(in_channels, out_channels[idx], K, **kwargs))\n",
    "            else:\n",
    "                self.en_layers.append(\n",
    "                    Enblock(out_channels[idx - 1], out_channels[idx], K,\n",
    "                            **kwargs))\n",
    "\n",
    "        # [height, arm_length, crotch_height, \n",
    "        #          chest_circumference, hip_circumference, waist_circumference,\n",
    "        self.en_layers.append(nn.Linear(self.num_vert * out_channels[-1], 6))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.en_layers):\n",
    "            if i != len(self.en_layers) - 1:\n",
    "                x = layer(x, self.edge_index[i], self.down_transform[i])\n",
    "            else:\n",
    "                x = x.view(-1, layer.weight.size(1))\n",
    "                predicted_val = layer(x)\n",
    "\n",
    "        return predicted_val\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ds_factor = [4, 4, 4, 4]\n",
    "edge_index_list, down_transform_list, up_transform_list = mesh_sampling_method(data_fp=data_dir,\n",
    "                                                                                   template_fp=template_fp,\n",
    "                                                                                   ds_factors=ds_factor,\n",
    "                                                                                   device=device)\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = [16, 16, 16, 32]\n",
    "K = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    batch_losses = []\n",
    "\n",
    "    # Iterate the dataloader\n",
    "    for idx, data in enumerate(dataloader, 0):\n",
    "        # Move tensor to the proper device(GPU)\n",
    "        x = data.x.to(device)\n",
    "        y = data.y.to(device)\n",
    "\n",
    "        # predict the value\n",
    "        predicted_value = model(x)\n",
    "        \n",
    "        loss = F.mse_loss(predicted_value, y, reduction='sum')\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch loss\n",
    "        batch_losses.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss / len(dataloader.dataset), batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, device, dataloader, fp):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    attributes_loss = torch.zeros(6).to(device)\n",
    "    \n",
    "    # height_std = 107.99, arm_std = 45.986, cross_std = 55.731, \n",
    "    # chest_std = 124.099, hip_std = 113.026, waist_std = 144.338\n",
    "    std_tensor = torch.tensor([107.99, 45.986, 55.731, 124.099, 113.026, 144.338]).to(device)\n",
    "\n",
    "    # Iterate the dataloader\n",
    "    for idx, data in enumerate(dataloader, 0):\n",
    "        # Move tensor to the proper device(GPU)\n",
    "        x = data.x.to(device)\n",
    "        y = data.y.to(device)\n",
    "\n",
    "        # predict the value\n",
    "        predicted_value = model(x)\n",
    "\n",
    "        # Calculate the loss. Since the reduction is 'none', the loss is of shape (batch_size, 10)\n",
    "        loss = F.mse_loss(predicted_value, y, reduction='none')\n",
    "        \n",
    "        # we sum it up according to each latent dimension\n",
    "        attributes_loss = attributes_loss + torch.sum(loss.detach(), dim=0)\n",
    "\n",
    "    # Calculate the average loss over the entire test set\n",
    "    test_loss = (attributes_loss / len(dataloader.dataset) * std_tensor).cpu().numpy()\n",
    "    \n",
    "    with open(fp, \"a+\") as f:\n",
    "        f.write(\"Height Loss: {:.5f}. Arm Loss: {:.5f}. Crotch Loss: {:.5f}.\\n\".format(test_loss[0], test_loss[1], test_loss[2]))\n",
    "        f.write(\"Chest Loss: {:.5f}. Hip Loss: {:.5f}. Waist Loss: {:.5f}\\n\".format(test_loss[3], test_loss[4], test_loss[5]))\n",
    "\n",
    "    print(\"Height Loss: {:.5f}. Arm Loss: {:.5f}. Crotch Loss: {:.5f}.\".format(test_loss[0], test_loss[1], test_loss[2]))\n",
    "    print(\"Chest Loss: {:.5f}. Hip Loss: {:.5f}. Waist Loss: {:.5f}\\n\".format(test_loss[3], test_loss[4], test_loss[5]))\n",
    "    \n",
    "\n",
    "    loss_dict = {'Height Loss': test_loss[0], 'Arm Loss': test_loss[1], 'Crotch Loss': test_loss[2],\n",
    "                 'Chest Loss': test_loss[3], 'Hip Loss': test_loss[4], 'Waist Loss': test_loss[5]}\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_loader, epochs, optimizer, device, fp):\n",
    "    total_batch_losses = []\n",
    "\n",
    "    height_losses, arm_losses, crotch_losses, chest_losses, hip_losses, waist_losses = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        t = time.time()\n",
    "        train_loss, loss_dict = train_epoch(model=model, device=device, dataloader=train_loader, optimizer=optimizer)\n",
    "        t_duration = time.time() - t\n",
    "        \n",
    "        with open(fp, 'a') as f:\n",
    "            f.write('Epoch: {:03d}/{:03d}, Train Loss: {:.5f}, Duration: {:.5f}\\n'.format(epoch, epochs, train_loss, t_duration))\n",
    "        print('Epoch: {:03d}/{:03d}, Train Loss: {:.5f}, Duration: {:.5f}'.format(epoch, epochs, train_loss, t_duration))\n",
    "        total_batch_losses = total_batch_losses + loss_dict\n",
    "        # Evaluate on test set\n",
    "        test_loss_dict = eval(model=model, device=device, dataloader=test_loader, fp=fp)\n",
    "        \n",
    "        height_losses.append(test_loss_dict['Height Loss'])\n",
    "        arm_losses.append(test_loss_dict['Arm Loss'])\n",
    "        crotch_losses.append(test_loss_dict['Crotch Loss'])\n",
    "        chest_losses.append(test_loss_dict['Chest Loss'])\n",
    "        hip_losses.append(test_loss_dict['Hip Loss'])\n",
    "        waist_losses.append(test_loss_dict['Waist Loss'])\n",
    "        \n",
    "    test_loss_dict = {'Height Loss': height_losses, 'Arm Loss': arm_losses, 'Crotch Loss': crotch_losses,\n",
    "                        'Chest Loss': chest_losses, 'Hip Loss': hip_losses, 'Waist Loss': waist_losses}\n",
    "\n",
    "    return total_batch_losses, test_loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plot functions\n",
    "\n",
    "def plot_train_loss(batch_losses, fp):\n",
    "    plt.plot(batch_losses)\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(fp)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def plot_test_loss(test_losses, ylabel, title, fp):\n",
    "    plt.plot(test_losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(fp)\n",
    "    plt.clf()\n",
    "\n",
    "def plot_together_loss(height_loss, weight_loss, gender_accuracy, fp):\n",
    "    # make a figure with two subplots, one for height, weight, the other for gender\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # for height and weight, add legend as well\n",
    "    ax1.plot(height_loss, label=\"height\")\n",
    "    ax1.plot(weight_loss, label=\"weight\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"Height and Weight Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    #for gender accuracy\n",
    "    ax2.plot(gender_accuracy)\n",
    "    ax2.set_title(\"Gender Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 001/050, Train Loss: 3.12338, Duration: 1.72365\n",
      "Height Loss: 11.49199. Arm Loss: 8.52810. Crotch Loss: 8.96580.\n",
      "Chest Loss: 49.62136. Hip Loss: 48.48763. Waist Loss: 57.39664\n",
      "\n",
      "Epoch: 002/050, Train Loss: 1.44191, Duration: 0.96886\n",
      "Height Loss: 9.99752. Arm Loss: 7.99568. Crotch Loss: 6.56875.\n",
      "Chest Loss: 21.50556. Hip Loss: 33.25151. Waist Loss: 25.75312\n",
      "\n",
      "Epoch: 003/050, Train Loss: 0.90816, Duration: 0.97856\n",
      "Height Loss: 8.08706. Arm Loss: 5.70454. Crotch Loss: 5.55415.\n",
      "Chest Loss: 14.27796. Hip Loss: 19.54047. Waist Loss: 19.89572\n",
      "\n",
      "Epoch: 004/050, Train Loss: 0.71543, Duration: 0.97727\n",
      "Height Loss: 7.10611. Arm Loss: 4.84554. Crotch Loss: 5.29361.\n",
      "Chest Loss: 11.16395. Hip Loss: 13.34611. Waist Loss: 16.84242\n",
      "\n",
      "Epoch: 005/050, Train Loss: 0.61679, Duration: 0.97881\n",
      "Height Loss: 7.51465. Arm Loss: 4.30802. Crotch Loss: 4.51789.\n",
      "Chest Loss: 9.76795. Hip Loss: 10.07273. Waist Loss: 15.93835\n",
      "\n",
      "Epoch: 006/050, Train Loss: 0.56214, Duration: 1.02666\n",
      "Height Loss: 6.87572. Arm Loss: 4.04997. Crotch Loss: 3.96985.\n",
      "Chest Loss: 8.62586. Hip Loss: 8.46574. Waist Loss: 15.08972\n",
      "\n",
      "Epoch: 007/050, Train Loss: 0.52472, Duration: 0.97164\n",
      "Height Loss: 6.55061. Arm Loss: 3.90095. Crotch Loss: 3.50531.\n",
      "Chest Loss: 7.80216. Hip Loss: 7.66183. Waist Loss: 14.23038\n",
      "\n",
      "Epoch: 008/050, Train Loss: 0.49690, Duration: 0.97225\n",
      "Height Loss: 6.25384. Arm Loss: 3.81234. Crotch Loss: 3.24598.\n",
      "Chest Loss: 7.16946. Hip Loss: 7.26270. Waist Loss: 13.39295\n",
      "\n",
      "Epoch: 009/050, Train Loss: 0.47454, Duration: 0.97079\n",
      "Height Loss: 6.12306. Arm Loss: 3.75789. Crotch Loss: 3.08444.\n",
      "Chest Loss: 6.74206. Hip Loss: 6.99286. Waist Loss: 12.76554\n",
      "\n",
      "Epoch: 010/050, Train Loss: 0.45608, Duration: 0.97102\n",
      "Height Loss: 6.04835. Arm Loss: 3.70996. Crotch Loss: 2.95642.\n",
      "Chest Loss: 6.41870. Hip Loss: 6.74237. Waist Loss: 12.24371\n",
      "\n",
      "Epoch: 011/050, Train Loss: 0.44003, Duration: 0.97569\n",
      "Height Loss: 5.99568. Arm Loss: 3.65754. Crotch Loss: 2.82063.\n",
      "Chest Loss: 6.15800. Hip Loss: 6.50283. Waist Loss: 11.77436\n",
      "\n",
      "Epoch: 012/050, Train Loss: 0.42557, Duration: 0.97662\n",
      "Height Loss: 5.94189. Arm Loss: 3.59977. Crotch Loss: 2.67550.\n",
      "Chest Loss: 5.93154. Hip Loss: 6.28398. Waist Loss: 11.34074\n",
      "\n",
      "Epoch: 013/050, Train Loss: 0.41229, Duration: 0.97846\n",
      "Height Loss: 5.88756. Arm Loss: 3.54336. Crotch Loss: 2.52911.\n",
      "Chest Loss: 5.73087. Hip Loss: 6.07677. Waist Loss: 10.94481\n",
      "\n",
      "Epoch: 014/050, Train Loss: 0.40002, Duration: 0.97636\n",
      "Height Loss: 5.83530. Arm Loss: 3.49359. Crotch Loss: 2.39375.\n",
      "Chest Loss: 5.55016. Hip Loss: 5.88055. Waist Loss: 10.59442\n",
      "\n",
      "Epoch: 015/050, Train Loss: 0.38872, Duration: 0.98436\n",
      "Height Loss: 5.78247. Arm Loss: 3.45130. Crotch Loss: 2.27248.\n",
      "Chest Loss: 5.38534. Hip Loss: 5.69300. Waist Loss: 10.29406\n",
      "\n",
      "Epoch: 016/050, Train Loss: 0.37831, Duration: 0.97757\n",
      "Height Loss: 5.72044. Arm Loss: 3.41359. Crotch Loss: 2.16114.\n",
      "Chest Loss: 5.23139. Hip Loss: 5.51222. Waist Loss: 10.03960\n",
      "\n",
      "Epoch: 017/050, Train Loss: 0.36867, Duration: 0.98407\n",
      "Height Loss: 5.64751. Arm Loss: 3.37816. Crotch Loss: 2.05665.\n",
      "Chest Loss: 5.08475. Hip Loss: 5.33832. Waist Loss: 9.82343\n",
      "\n",
      "Epoch: 018/050, Train Loss: 0.35976, Duration: 0.98117\n",
      "Height Loss: 5.57254. Arm Loss: 3.34443. Crotch Loss: 1.96163.\n",
      "Chest Loss: 4.94366. Hip Loss: 5.17119. Waist Loss: 9.63913\n",
      "\n",
      "Epoch: 019/050, Train Loss: 0.35155, Duration: 0.98112\n",
      "Height Loss: 5.50518. Arm Loss: 3.31143. Crotch Loss: 1.87980.\n",
      "Chest Loss: 4.80696. Hip Loss: 5.01136. Waist Loss: 9.48132\n",
      "\n",
      "Epoch: 020/050, Train Loss: 0.34402, Duration: 0.98858\n",
      "Height Loss: 5.44547. Arm Loss: 3.27672. Crotch Loss: 1.81061.\n",
      "Chest Loss: 4.67332. Hip Loss: 4.86021. Waist Loss: 9.34415\n",
      "\n",
      "Epoch: 021/050, Train Loss: 0.33704, Duration: 0.97989\n",
      "Height Loss: 5.38469. Arm Loss: 3.23812. Crotch Loss: 1.74971.\n",
      "Chest Loss: 4.54155. Hip Loss: 4.71891. Waist Loss: 9.22282\n",
      "\n",
      "Epoch: 022/050, Train Loss: 0.33044, Duration: 0.98060\n",
      "Height Loss: 5.31294. Arm Loss: 3.19542. Crotch Loss: 1.69242.\n",
      "Chest Loss: 4.41131. Hip Loss: 4.58797. Waist Loss: 9.11383\n",
      "\n",
      "Epoch: 023/050, Train Loss: 0.32411, Duration: 0.98207\n",
      "Height Loss: 5.22620. Arm Loss: 3.15005. Crotch Loss: 1.63673.\n",
      "Chest Loss: 4.28291. Hip Loss: 4.46733. Waist Loss: 9.01459\n",
      "\n",
      "Epoch: 024/050, Train Loss: 0.31804, Duration: 0.98242\n",
      "Height Loss: 5.12696. Arm Loss: 3.10439. Crotch Loss: 1.58379.\n",
      "Chest Loss: 4.15715. Hip Loss: 4.35639. Waist Loss: 8.92303\n",
      "\n",
      "Epoch: 025/050, Train Loss: 0.31229, Duration: 0.97891\n",
      "Height Loss: 5.02178. Arm Loss: 3.06159. Crotch Loss: 1.53664.\n",
      "Chest Loss: 4.03548. Hip Loss: 4.25459. Waist Loss: 8.83806\n",
      "\n",
      "Epoch: 026/050, Train Loss: 0.30696, Duration: 0.97927\n",
      "Height Loss: 4.91844. Arm Loss: 3.02512. Crotch Loss: 1.49865.\n",
      "Chest Loss: 3.92001. Hip Loss: 4.16137. Waist Loss: 8.75943\n",
      "\n",
      "Epoch: 027/050, Train Loss: 0.30215, Duration: 0.97950\n",
      "Height Loss: 4.82404. Arm Loss: 2.99762. Crotch Loss: 1.47194.\n",
      "Chest Loss: 3.81316. Hip Loss: 4.07533. Waist Loss: 8.68703\n",
      "\n",
      "Epoch: 028/050, Train Loss: 0.29789, Duration: 0.98615\n",
      "Height Loss: 4.74181. Arm Loss: 2.97938. Crotch Loss: 1.45550.\n",
      "Chest Loss: 3.71654. Hip Loss: 3.99495. Waist Loss: 8.62063\n",
      "\n",
      "Epoch: 029/050, Train Loss: 0.29414, Duration: 0.97837\n",
      "Height Loss: 4.66951. Arm Loss: 2.96786. Crotch Loss: 1.44543.\n",
      "Chest Loss: 3.63091. Hip Loss: 3.91888. Waist Loss: 8.56007\n",
      "\n",
      "Epoch: 030/050, Train Loss: 0.29080, Duration: 0.97755\n",
      "Height Loss: 4.60148. Arm Loss: 2.95935. Crotch Loss: 1.43689.\n",
      "Chest Loss: 3.55610. Hip Loss: 3.84667. Waist Loss: 8.50457\n",
      "\n",
      "Epoch: 031/050, Train Loss: 0.28775, Duration: 0.97866\n",
      "Height Loss: 4.53284. Arm Loss: 2.95080. Crotch Loss: 1.42595.\n",
      "Chest Loss: 3.49121. Hip Loss: 3.77814. Waist Loss: 8.45314\n",
      "\n",
      "Epoch: 032/050, Train Loss: 0.28489, Duration: 0.98239\n",
      "Height Loss: 4.46151. Arm Loss: 2.94076. Crotch Loss: 1.41061.\n",
      "Chest Loss: 3.43526. Hip Loss: 3.71302. Waist Loss: 8.40504\n",
      "\n",
      "Epoch: 033/050, Train Loss: 0.28212, Duration: 0.97956\n",
      "Height Loss: 4.38849. Arm Loss: 2.92941. Crotch Loss: 1.39083.\n",
      "Chest Loss: 3.38733. Hip Loss: 3.65106. Waist Loss: 8.35983\n",
      "\n",
      "Epoch: 034/050, Train Loss: 0.27941, Duration: 0.97864\n",
      "Height Loss: 4.31705. Arm Loss: 2.91789. Crotch Loss: 1.36820.\n",
      "Chest Loss: 3.34656. Hip Loss: 3.59191. Waist Loss: 8.31746\n",
      "\n",
      "Epoch: 035/050, Train Loss: 0.27676, Duration: 0.98405\n",
      "Height Loss: 4.25116. Arm Loss: 2.90752. Crotch Loss: 1.34495.\n",
      "Chest Loss: 3.31202. Hip Loss: 3.53555. Waist Loss: 8.27763\n",
      "\n",
      "Epoch: 036/050, Train Loss: 0.27418, Duration: 0.97846\n",
      "Height Loss: 4.19371. Arm Loss: 2.89911. Crotch Loss: 1.32273.\n",
      "Chest Loss: 3.28275. Hip Loss: 3.48181. Waist Loss: 8.23989\n",
      "\n",
      "Epoch: 037/050, Train Loss: 0.27173, Duration: 0.97243\n",
      "Height Loss: 4.14574. Arm Loss: 2.89270. Crotch Loss: 1.30215.\n",
      "Chest Loss: 3.25783. Hip Loss: 3.43052. Waist Loss: 8.20349\n",
      "\n",
      "Epoch: 038/050, Train Loss: 0.26941, Duration: 0.98237\n",
      "Height Loss: 4.10677. Arm Loss: 2.88762. Crotch Loss: 1.28295.\n",
      "Chest Loss: 3.23641. Hip Loss: 3.38146. Waist Loss: 8.16783\n",
      "\n",
      "Epoch: 039/050, Train Loss: 0.26724, Duration: 0.97481\n",
      "Height Loss: 4.07526. Arm Loss: 2.88291. Crotch Loss: 1.26442.\n",
      "Chest Loss: 3.21782. Hip Loss: 3.33461. Waist Loss: 8.13240\n",
      "\n",
      "Epoch: 040/050, Train Loss: 0.26520, Duration: 0.99590\n",
      "Height Loss: 4.04938. Arm Loss: 2.87773. Crotch Loss: 1.24590.\n",
      "Chest Loss: 3.20155. Hip Loss: 3.28992. Waist Loss: 8.09690\n",
      "\n",
      "Epoch: 041/050, Train Loss: 0.26326, Duration: 0.99601\n",
      "Height Loss: 4.02759. Arm Loss: 2.87166. Crotch Loss: 1.22708.\n",
      "Chest Loss: 3.18711. Hip Loss: 3.24712. Waist Loss: 8.06118\n",
      "\n",
      "Epoch: 042/050, Train Loss: 0.26142, Duration: 0.96959\n",
      "Height Loss: 4.00889. Arm Loss: 2.86471. Crotch Loss: 1.20810.\n",
      "Chest Loss: 3.17411. Hip Loss: 3.20605. Waist Loss: 8.02505\n",
      "\n",
      "Epoch: 043/050, Train Loss: 0.25967, Duration: 0.97189\n",
      "Height Loss: 3.99283. Arm Loss: 2.85716. Crotch Loss: 1.18932.\n",
      "Chest Loss: 3.16231. Hip Loss: 3.16640. Waist Loss: 7.98845\n",
      "\n",
      "Epoch: 044/050, Train Loss: 0.25799, Duration: 0.97805\n",
      "Height Loss: 3.97932. Arm Loss: 2.84929. Crotch Loss: 1.17118.\n",
      "Chest Loss: 3.15142. Hip Loss: 3.12777. Waist Loss: 7.95142\n",
      "\n",
      "Epoch: 045/050, Train Loss: 0.25638, Duration: 0.97240\n",
      "Height Loss: 3.96852. Arm Loss: 2.84133. Crotch Loss: 1.15397.\n",
      "Chest Loss: 3.14122. Hip Loss: 3.08977. Waist Loss: 7.91410\n",
      "\n",
      "Epoch: 046/050, Train Loss: 0.25485, Duration: 0.98380\n",
      "Height Loss: 3.96062. Arm Loss: 2.83345. Crotch Loss: 1.13795.\n",
      "Chest Loss: 3.13161. Hip Loss: 3.05210. Waist Loss: 7.87657\n",
      "\n",
      "Epoch: 047/050, Train Loss: 0.25338, Duration: 0.97889\n",
      "Height Loss: 3.95595. Arm Loss: 2.82584. Crotch Loss: 1.12344.\n",
      "Chest Loss: 3.12252. Hip Loss: 3.01443. Waist Loss: 7.83909\n",
      "\n",
      "Epoch: 048/050, Train Loss: 0.25199, Duration: 0.98269\n",
      "Height Loss: 3.95479. Arm Loss: 2.81872. Crotch Loss: 1.11078.\n",
      "Chest Loss: 3.11391. Hip Loss: 2.97659. Waist Loss: 7.80205\n",
      "\n",
      "Epoch: 049/050, Train Loss: 0.25065, Duration: 0.99927\n",
      "Height Loss: 3.95734. Arm Loss: 2.81231. Crotch Loss: 1.10042.\n",
      "Chest Loss: 3.10576. Hip Loss: 2.93853. Waist Loss: 7.76582\n",
      "\n",
      "Epoch: 050/050, Train Loss: 0.24938, Duration: 0.98495\n",
      "Height Loss: 3.96363. Arm Loss: 2.80679. Crotch Loss: 1.09280.\n",
      "Chest Loss: 3.09809. Hip Loss: 2.90050. Waist Loss: 7.73084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GNN_together = GraphPredictor(in_channels=in_channels, \n",
    "                               out_channels=out_channels, \n",
    "                               edge_index=edge_index_list,\n",
    "                               down_transform=down_transform_list,\n",
    "                               K=K).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(GNN_together.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 50\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "\n",
    "log_fp = osp.join(\"predictor_network\", \"log.txt\")\n",
    "\n",
    "batch_losses, test_loss_dict = run(GNN_together, train_loader, epochs, optimizer, device, fp=log_fp)\n",
    "\n",
    "PATH = osp.join(\"predictor_network\", \"predictor.pth\")\n",
    "torch.save(GNN_together.state_dict(), PATH)\n",
    "\n",
    "# TODO: plot the results\n",
    "# plot the results\n",
    "# plot_train_loss(batch_losses, \"predictor_network/together/train_loss.pdf\")\n",
    "# plot_together_loss(height_test_losses, weight_test_losses, gender_test_losses, \"predictor_network/together/test_loss.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Predictor\n",
    "\n",
    "The predictor network is fully MLP. After trying different pairs of hyper-parameters, the result is still not that good. After around 20-30 epochs, the training loss doesn't have much difference. But the testing loss keeps changing up and down. \n",
    "\n",
    "I think we need to change the model structure. Maybe we can try to use the graph convolutional network. In this Jupyter Notebook, we will only list the training process of CoMA based predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetworkBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, leaky_relu_slope=0.2):\n",
    "        super(MLPNetworkBlock, self).__init__()\n",
    "        self.linear_layer = nn.Linear(in_channel, out_channel)\n",
    "        self.leaky_relu_slope = leaky_relu_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_layer(x)\n",
    "        out = F.leaky_relu(input=out, negative_slope=self.leaky_relu_slope)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(len(out_channels)):\n",
    "            if idx == 0:\n",
    "                self.layers.append(MLPNetworkBlock(in_channel=in_channel, out_channel=out_channels[0]))\n",
    "            else:\n",
    "                self.layers.append(MLPNetworkBlock(in_channel=out_channels[idx-1], out_channel=out_channels[idx]))\n",
    "\n",
    "        # Linear layer to generate predicted value\n",
    "        self.linear_layer = nn.Linear(out_channels[-1], 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        # get the predicted value\n",
    "        z = self.linear_layer(x)\n",
    "        return z\n",
    "\n",
    "# define the model\n",
    "\n",
    "# in_channel = meshdata.num_nodes * 3\n",
    "# # out_channels = [2048, 512, 128, 64, 16] Loss around 10 \n",
    "# # out_channels = [1024, 256, 64, 16, 4] Loss around 10\n",
    "# out_channels = [4096, 1024, 256, 64, 16] \n",
    "# predictor_network = MLPPredictor(in_channel, out_channels).cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "body_coma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
