{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import yaml\n",
    "import os.path as osp\n",
    "\n",
    "import igl\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.ops import cot_laplacian\n",
    "\n",
    "from preprocess import mesh_sampling_method\n",
    "from dataset import MeshData\n",
    "from models import VAE_coma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce7ad5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the training and testing dataset...\n",
      "Normalization Done!\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config/general_config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# set the device, we can just assume we are using single GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = config[\"dataset\"]\n",
    "template = config[\"template\"]\n",
    "data_dir = osp.join('data', dataset)\n",
    "template_fp = osp.join('template', template)\n",
    "\n",
    "# get the up/down sampling matrix\n",
    "ds_factor = config[\"ds_factor\"]\n",
    "edge_index_list, down_transform_list, up_transform_list = mesh_sampling_method(data_fp=data_dir,\n",
    "                                                                                template_fp=template_fp,\n",
    "                                                                                ds_factors=ds_factor,\n",
    "                                                                                device=device)\n",
    "\n",
    "# create the model\n",
    "in_channels = config[\"model\"][\"in_channels\"]\n",
    "out_channels = config[\"model\"][\"out_channels\"]\n",
    "latent_channels = config[\"model\"][\"latent_channels\"]\n",
    "K = config[\"model\"][\"K\"]\n",
    "\n",
    "model = VAE_coma(in_channels = in_channels,\n",
    "                out_channels = out_channels,\n",
    "                latent_channels = latent_channels,\n",
    "                edge_index = edge_index_list,\n",
    "                down_transform = down_transform_list,\n",
    "                up_transform = up_transform_list,\n",
    "                K=K).to(device)\n",
    "\n",
    "# get the mean and std of the CAESAR dataset(Traing set)\n",
    "CAESAR_meshdata = MeshData(root=data_dir, template_fp=template_fp)\n",
    "# of shape (10002, 3). is torch.Tensor\n",
    "mean = CAESAR_meshdata.mean\n",
    "std = CAESAR_meshdata.std\n",
    "\n",
    "# load the template mesh\n",
    "template_v, template_f = igl.read_triangle_mesh(template_fp)\n",
    "\n",
    "# convert it into float32\n",
    "# as in cot_laplacian, it only supports toch.float32. inv_areas = torch.zeros(V, dtype=torch.float32, device=verts.device)\n",
    "template_v = template_v.astype('float32')\n",
    "# convert it into a list of pytorch Tensor.\n",
    "template_v = torch.from_numpy(template_v).cuda()\n",
    "template_f = torch.from_numpy(template_f).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957cf2b",
   "metadata": {},
   "source": [
    "# Decoder function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee63b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_func(x, model):\n",
    "    num_layers = len(model.de_layers)\n",
    "    num_deblocks = num_layers - 2\n",
    "    for i, layer in enumerate(model.de_layers):\n",
    "        if i == 0:\n",
    "            x = layer(x)\n",
    "            x = x.view(-1, model.num_vert, model.out_channels[-1])\n",
    "        elif i != num_layers - 1:\n",
    "            x = layer(x, model.edge_index[num_deblocks - i],\n",
    "                        model.up_transform[num_deblocks - i])\n",
    "        else:\n",
    "            # last layer\n",
    "            x = layer(x, model.edge_index[0])\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c15a9",
   "metadata": {},
   "source": [
    "# The Laplacian function to compute the Laplacian for each mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7ec27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(vertex: torch.Tensor, faces: torch.Tensor, type: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the laplacian of a mesh.\n",
    "    Args:\n",
    "        vertex: FloatTensor of shape (V, 3) giving vertex positions for V vertices.\n",
    "        faces: LongTensor of shape (F, 3) giving faces.\n",
    "        type: String giving the type of laplacian to compute. Must be either 'mean' or 'vertex'.\n",
    "              mean: compute the mean laplacian value for all vertices.\n",
    "              vertex: compute the laplacian value for each vertex.\n",
    "    Returns:\n",
    "        laplacian: FloatTensor of shape (V, 1) giving the laplacian matrix for the mesh.\n",
    "                   OR, retuen the mean laplacian value for all vertices.\n",
    "    \"\"\"\n",
    "    if type not in ['mean', 'vertex', 'vector']:\n",
    "        raise ValueError('type must be one of mean, vertex or vector')\n",
    "\n",
    "    # Compute the cotangent weights.\n",
    "    L, inv_areas = cot_laplacian(vertex, faces)\n",
    "\n",
    "    # NOTE: The diagonal entries of the cotangent weights matrix returned by the method are 0\n",
    "    #       and need to be computed with the negative sum of the weights in each row.\n",
    "    L_sum = torch.sparse.sum(L, dim=1).to_dense().view(-1, 1)\n",
    "\n",
    "    # As shown in my thesis, the laplacian is given by: L_loss = L * M^{-1} * V, \n",
    "    # However, since the diagonal is zero, we modify it like: L_loss = (L - L_sum) * M^{-1} * V, \n",
    "    # where L is the cotangent weights matrix, \n",
    "    #       M is the mass matrix, with diagonal entries being 1/3 areas of the vertices,\n",
    "    #       V is the vertex positions.\n",
    "    \n",
    "    # NOTE: I have no idea where the 0.25 comes from.\n",
    "    #       In my opinion, it should be the weight should be 1/2 (cot alpha_{ij} + cot beta_{ij}).\n",
    "    #       So, the coefficient should be 0.5. \n",
    "    mass_matrix_inv = 0.25 * inv_areas\n",
    "\n",
    "    loss = (L.mm(vertex) - L_sum * vertex) * mass_matrix_inv\n",
    "\n",
    "    laplacian_loss = torch.norm(loss, dim=1)\n",
    "\n",
    "    if type == 'mean':\n",
    "        return torch.mean(laplacian_loss)\n",
    "    elif type == 'vertex':\n",
    "        return laplacian_loss\n",
    "    elif type == 'vector':\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c60c7c",
   "metadata": {},
   "source": [
    "# generate mesh samples with direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e2f981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a banch of meshes samples from the latent space\n",
    "def generate_mesh_samples(model_path: str, sample_vals: list, model, mean, std, template_fp: str, model_name: str):\n",
    "    \"\"\"_summary_\n",
    "        generate meshes from the latent values\n",
    "        save the meshes to the result folder\n",
    "\n",
    "    Args:\n",
    "        model_path (str): the path to the model file\n",
    "        sample_vals (list): the list of latent values we want to sample\n",
    "        model (VAE_coma): the model\n",
    "        mean (torch.Tensor): of shape (10002, 3). The mean of the CAESAR dataset.\n",
    "        std (torch.Tensor): of shape (10002, 3). The std of the CAESAR dataset.\n",
    "        template_fp (str): the path to the template mesh\n",
    "        model_name (str): the name of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name not in ['vanilla_vae', 'tc_vae', 'beta_annealing', 'deriv']:\n",
    "        raise ValueError('model_name must be one of vanilla_vae, tc_vae, beta_annealing, deriv')\n",
    "\n",
    "    # Compute the Laplacian loss of the template mesh\n",
    "    # Compute the Laplacian loss of the zeros latent vector\n",
    "\n",
    "    # load the template mesh\n",
    "    template_v, template_f = igl.read_triangle_mesh(template_fp)\n",
    "    # create the template trimesh\n",
    "    template_trimesh = trimesh.Trimesh(vertices=template_v, faces=template_f)\n",
    "    # compute the vertex normals\n",
    "    template_vertex_normals = trimesh.smoothing.get_vertices_normals(template_trimesh)\n",
    "    # convert it into float32\n",
    "    # as in cot_laplacian, it only supports toch.float32. inv_areas = torch.zeros(V, dtype=torch.float32, device=verts.device)\n",
    "    template_v = template_v.astype('float32')\n",
    "    # convert it into a list of pytorch Tensor.\n",
    "    template_v = torch.from_numpy(template_v).cuda()\n",
    "    template_f = torch.from_numpy(template_f).cuda()\n",
    "    # compute the laplacian loss of the template mesh\n",
    "    template_laplacian_loss_vector = laplacian(template_v, template_f, 'vector')\n",
    "    template_laplacian_loss_vector = template_laplacian_loss_vector.cpu().numpy()\n",
    "    # compute the laplacian loss of the template mesh\n",
    "    template_laplacian_loss = np.sum(template_laplacian_loss_vector * template_vertex_normals, axis=1)\n",
    "\n",
    "    \n",
    "    # move the mean and std to the device\n",
    "    mean = mean.to(device)\n",
    "    std = std.to(device)\n",
    "\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # create a zero tensor of shape (1, 8) as mean\n",
    "    mean_latent = torch.zeros((1, 8)).cuda()\n",
    "    # decode the mean\n",
    "    mean_v = decode_func(mean_latent, model)\n",
    "    # convert to numpy array\n",
    "    mean_v = mean_v.detach()\n",
    "    mean_v = mean_v * std + mean\n",
    "    mean_v = mean_v.reshape(-1, 3)\n",
    "\n",
    "    # get the laplacian loss of the mean latent vector\n",
    "    mean_laplacian_loss_vector = laplacian(mean_v, template_f, 'vector')\n",
    "    mean_laplacian_loss_vector = mean_laplacian_loss_vector.cpu().numpy()\n",
    "\n",
    "    # create a mesh and use it compute the vertex normals\n",
    "    mean_trimesh = trimesh.Trimesh(vertices=mean_v.cpu().numpy(), faces=template_f.cpu().numpy())\n",
    "    mean_vertex_normals = trimesh.smoothing.get_vertices_normals(mean_trimesh)\n",
    "\n",
    "    # compute the laplacian loss of the template mesh\n",
    "    mean_laplacian_loss = np.sum(mean_laplacian_loss_vector * mean_vertex_normals, axis=1)\n",
    "\n",
    "    #\n",
    "    # Now, we generate the meshes from the latent vectors and compute its laplacian loss\n",
    "    # \n",
    "\n",
    "    latent_vectors = torch.zeros((len(sample_vals) * 8, 8))\n",
    "\n",
    "    for i in range(8):\n",
    "        latent_vectors[i * len(sample_vals): (i + 1) * len(sample_vals), i] = torch.Tensor(sample_vals)\n",
    "\n",
    "    # get the total number of latent vectors\n",
    "    n = latent_vectors.shape[0]\n",
    "\n",
    "    for i in tqdm(range(8)):\n",
    "\n",
    "        for j in range(len(sample_vals)):\n",
    "\n",
    "            # get the latent vector\n",
    "            n = i * len(sample_vals) + j\n",
    "\n",
    "            latent_vector = torch.Tensor(latent_vectors[n]).cuda()\n",
    "\n",
    "            # decode the latent values\n",
    "            # of shape (n_samples, 10002, 3)\n",
    "            v = decode_func(latent_vector, model)\n",
    "            # convert to numpy array\n",
    "            v = v.detach()\n",
    "            # denormalize the vertices\n",
    "            v = v * std + mean\n",
    "            v = v.reshape(-1, 3)\n",
    "\n",
    "            # compute the laplacian loss\n",
    "            laplacian_loss_vector = laplacian(v, template_f, 'vector')\n",
    "            laplacian_loss_vector = laplacian_loss_vector.cpu().numpy()\n",
    "            # move the v to cpu\n",
    "            v = v.cpu().numpy()\n",
    "\n",
    "            # move the template_f to cpu\n",
    "            template_f = template_f.cpu().numpy()\n",
    "            \n",
    "            # save the mesh \n",
    "            trimeshMesh = trimesh.Trimesh(vertices=v, faces=template_f)\n",
    "            vertex_normals = trimesh.smoothing.get_vertices_normals(trimeshMesh)\n",
    "            laplacian_loss = np.sum(laplacian_loss_vector * vertex_normals, axis=1)\n",
    "            mesh = pv.wrap(trimeshMesh)\n",
    "            mesh[\"lap_loss\"] = laplacian_loss\n",
    "            mesh[\"lap_from_mean\"] = laplacian_loss - mean_laplacian_loss\n",
    "            mesh[\"lap_from_template\"] = laplacian_loss - template_laplacian_loss\n",
    "            mesh.save(osp.join('result', 'laplacian', model_name, \"meshes\", \"mesh{}{:02d}.vtk\".format(i, j)))\n",
    "\n",
    "            # convert the template_f back to torch.Tensor\n",
    "            template_f = torch.from_numpy(template_f).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce19ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lap(model_path: str, sample_vals: np.ndarray, mean: torch.Tensor, std: torch.Tensor, faces: torch.Tensor, model):\n",
    "    \"\"\"_summary_\n",
    "        load the model from model_path\n",
    "        sample several points from the latent space\n",
    "        for each corresponding mesh, return the laplacian loss(type = 'vertex')\n",
    "\n",
    "    Args:\n",
    "        model_path (str): the path to the model file\n",
    "        sample_vals (np.ndarray): of shape (n_samples, ). We would sample n_samples from the latent space. And use the same vals for all 8 dimensions.\n",
    "        mean (torch.Tensor): of shape (10002, 3). is torch.Tensor\n",
    "        std (torch.Tensor): of shape (10002, 3). is torch.Tensor\n",
    "        faces (torch.Tensor): of shape (10002, 3). The faces of the template mesh.\n",
    "        model (VAE_coma): the model\n",
    "    \"\"\"\n",
    "\n",
    "    mean = mean.to(device)\n",
    "    std = std.to(device)\n",
    "    \n",
    "    # Load the model\n",
    "    model_params = torch.load(model_path)\n",
    "    model.load_state_dict(model_params)\n",
    "\n",
    "    # Sample from the latent space\n",
    "    # create a tensor of shape (n_samples, 8)\n",
    "    lap_vals = np.zeros((10002, sample_vals.shape[0], 8))\n",
    "\n",
    "    # for each latent dimension\n",
    "    for i in tqdm(range(8)):\n",
    "        latent_val = torch.zeros((sample_vals.shape[0], 8))\n",
    "\n",
    "        latent_val[:, i] = torch.Tensor(sample_vals)\n",
    "        latent_val = latent_val.to(device) \n",
    "        \n",
    "        # decode the latent values\n",
    "        # of shape (n_samples, 10002, 3)\n",
    "        v = decode_func(latent_val, model)\n",
    "\n",
    "        v = v.detach() \n",
    "        v = v * std + mean\n",
    "\n",
    "        for j in range(sample_vals.shape[0]):\n",
    "            # get the laplacian loss for each mesh\n",
    "            laplacian_loss = laplacian(v[j], faces, type='vertex')\n",
    "            lap_vals[:, j, i] = laplacian_loss.cpu().numpy()\n",
    "\n",
    "    return lap_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0114e02",
   "metadata": {},
   "source": [
    "# Plot function for plotting result for each latent variable change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87e362aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "# TODO: Change the y-axis limit. The specific value should be determined by the dataset.\n",
    "def plot_latent(filename: str, sample_vals: np.ndarray, lap_vals: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "        declare a plot of 8 subplots, 8 rows and 1 column\n",
    "        each subplot is a dot plot of the corresponding latent dimension\n",
    "        the x-axis is the latent value, the y-axis is the height\n",
    "        the figure size is tight to the subplots\n",
    "    Args:\n",
    "        filename (str): the file name to save the plot.\n",
    "        sample_vals (np.ndarray): of shape (n_samples, ). We would sample n_samples from the latent space. And use the same vals for all 8 dimensions.\n",
    "        lap_vals (np.ndarray): of shape (n_samples, 8). 8 is the number of latent dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the dimensions match up correctly\n",
    "    if len(sample_vals) != lap_vals.shape[0] or lap_vals.shape[1] != 8:\n",
    "        raise ValueError(\"Input dimensions are mismatched!\")\n",
    "\n",
    "    fig, axs = plt.subplots(8, 1, figsize=(5, 28), tight_layout=True)\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        # Plot each latent dimension\n",
    "        ax.scatter(sample_vals, lap_vals[:, i], marker='o')\n",
    "        \n",
    "        # set the x-axis limit to be between -1 and 1\n",
    "        # set the y-axis limit to be between 1.5 and 2\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.set_ylim(12, 19)\n",
    "        \n",
    "        ax.set_title(\"Latent dimension {}\".format(i))\n",
    "\n",
    "    # Save the figure to the given filename\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153afaa",
   "metadata": {},
   "source": [
    "# The Laplacian Loss on template mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97812326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max loss is 416.103\n",
      "The min loss is -785.334\n",
      "The median loss is -3.422\n",
      "The std loss is 25.637\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_lap_loss_mean = laplacian(template_v, template_f, 'mean')\n",
    "# print(f\"The mean loss is {temp_lap_loss_mean:.3f}\")\n",
    "\n",
    "temp_lap_loss_vertex = laplacian(template_v, template_f, 'vertex')\n",
    "# print(\"The max loss is {:.3f}\".format(torch.max(temp_lap_loss_vertex).item()))\n",
    "# print(\"The median loss is {:.3f}\".format(torch.median(temp_lap_loss_vertex).item()))\n",
    "# print(\"The std loss is {:.3f}\".format(torch.std(temp_lap_loss_vertex).item()))\n",
    "\n",
    "# plt.hist(temp_lap_loss_vertex .cpu().numpy(), bins=100)\n",
    "\n",
    "temp_v, temp_f = igl.read_triangle_mesh(template_fp)\n",
    "temp_mesh = trimesh.Trimesh(vertices=temp_v, faces=temp_f)\n",
    "vertex_normals = trimesh.smoothing.get_vertices_normals(temp_mesh)\n",
    "temp_lap_loss_vector = laplacian(template_v, template_f, 'vector').cpu().numpy()\n",
    "\n",
    "lap = np.sum(temp_lap_loss_vector * vertex_normals, axis=1)\n",
    "temp_mesh = pv.wrap(temp_mesh)\n",
    "temp_mesh[\"lap_loss\"] = lap\n",
    "temp_mesh.save(osp.join('result', 'laplacian', 'template.vtk'))\n",
    "\n",
    "plt.hist(lap, bins=100)\n",
    "\n",
    "print(\"The max loss is {:.3f}\".format(np.max(lap)))\n",
    "print(\"The min loss is {:.3f}\".format(np.min(lap)))\n",
    "print(\"The median loss is {:.3f}\".format(np.median(lap)))\n",
    "print(\"The std loss is {:.3f}\".format(np.std(lap)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31daa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_v, temp_f = igl.read_triangle_mesh(template_fp)\n",
    "temp_mesh = trimesh.Trimesh(vertices=temp_v, faces=temp_f)\n",
    "temp_lap_loss_vertex = laplacian(template_v, template_f, 'vertex').cpu().numpy()\n",
    "\n",
    "temp_mesh = pv.wrap(temp_mesh)\n",
    "temp_mesh[\"lap_loss\"] = temp_lap_loss_vertex\n",
    "temp_mesh_diff = np.abs(temp_lap_loss_vertex - temp_lap_loss_vertex)\n",
    "temp_mesh[\"temp_diff\"] = temp_mesh_diff\n",
    "temp_mesh.save(osp.join('result', 'laplacian', 'template_mean.vtk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de7ba254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  9.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"out/vae_coma/trainer12/20231220-123538/model.pth\" # our method\n",
    "# model_path = \"out/vae_coma/trainer13/20240119-003904/model.pth\" # our method without Laplacian loss\n",
    "# model_path = \"out/vae_coma/trainer14/20240119-090527/model.pth\" # our method without AWL\n",
    "model_path = \"out/vae_coma/trainer15/20240119-193650/model.pth\" # our method without disentanglement\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "sample_vals = np.linspace(-1, 1, 50)\n",
    "lap_loss = get_lap(model_path, sample_vals, mean, std, template_f, model)\n",
    "\n",
    "\n",
    "# write the printed info into a file\n",
    "with open(\"result/ablation/laplacian/wo_disentanglement/laplacian.txt\", \"w\") as f:\n",
    "    print(\"The mean loss for our method is {:.3f}\".format(np.mean(lap_loss)), file=f)\n",
    "    print(\"The max loss for our method is {:.3f}\".format(np.max(lap_loss)), file=f)\n",
    "    print(\"The median loss for our method is {:.3f}\".format(np.median(lap_loss)), file=f)\n",
    "    print(\"The std loss for our method is {:.3f}\".format(np.std(lap_loss)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "669a98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "latent_vector = torch.zeros((1, 8)).cuda()\n",
    "v = decode_func(latent_vector, model)\n",
    "# convert to numpy array\n",
    "v = v.detach().cpu()\n",
    "# denormalize the vertices\n",
    "v = v * std + mean\n",
    "v = v.reshape(-1, 3)\n",
    "\n",
    "temp_v, temp_f = igl.read_triangle_mesh(template_fp)\n",
    "our_method_mesh = trimesh.Trimesh(vertices=v, faces=temp_f)\n",
    "v = v.cuda()\n",
    "our_method_lap_loss_vertex = laplacian(v, template_f, 'vertex').cpu().numpy()\n",
    "our_method_lap_loss_diff = np.abs(our_method_lap_loss_vertex - temp_lap_loss_vertex)\n",
    "our_method_mean_mesh = pv.wrap(our_method_mesh)\n",
    "our_method_mean_mesh[\"lap_loss\"] = our_method_lap_loss_vertex\n",
    "our_method_mean_mesh[\"lap_loss_diff\"] = our_method_lap_loss_diff\n",
    "our_method_mean_mesh.save(osp.join('result', 'ablation', 'laplacian', 'wo_disentanglement', 'mean.vtk'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "body_coma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
